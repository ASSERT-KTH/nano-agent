defaults:
  - ppo_trainer
  - _self_ 

trainer:
  # Training configuration
  val_before_train: false   # skip the pre-training sanity pass
  test_freq: 999999999      # or -1 with nightly builds
  save_freq: 20
  total_epochs: 10

  critic_warmup: 0
  
  # Project tracking
  project_name: nano-agent-grpo
  experiment_name: qwen3-8b-grpo-run
  default_local_dir: output/nano8b_grpo
  
  # Distributed training
  n_gpus_per_node: 2
  nnodes: 1
  
  # Logging
  logger: ["console"]

# Data configuration
data:
  train_files: swe_gym_verl.parquet
  val_files: dummy.parquet
  prompt_key: messages
  reward_model_key: patch   
  train_batch_size: 16
  
  # Token limits (explicit as mentioned in brief)
  max_prompt_length: 1024
  max_response_length: 7168
  filter_overlong_prompts: true
  truncation: 'error'

# Model and training configuration
actor_rollout_ref:
  # Hybrid engine for 3D weight sharding
  hybrid_engine: true

  model:
    path: Qwen/Qwen3-8B
    trust_remote_code: true

    # LoRA configuration (as specified in brief)
    lora_rank: 32
    lora_alpha: 64
    lora_dropout: 0.1
    target_modules: ["q_proj", "k_proj", "v_proj", "o_proj"]
    load_format: "safetensors"
    
    # Memory optimization
    gpu_memory_utilization: 0.9
    enable_gradient_checkpointing: true
    use_remove_padding: true
  
  # Actor (learner) configuration
  actor:
    strategy: fsdp2
    optim:
      lr: 5e-6
      weight_decay: 0.01
    
    # GRPO-specific settings
    use_kl_loss: true
    kl_loss_coef: 0.001
    kl_loss_type: low_var_kl
    entropy_coeff: 0.0
    
    # Batch sizes
    ppo_mini_batch_size: 4
    ppo_micro_batch_size_per_gpu: 4
    ppo_batch_size: 16
    
    # FSDP configuration
    fsdp_config:
      param_offload: false
      optimizer_offload: false
  
  # Reference model configuration
  ref:
    strategy: fsdp2
    log_prob_micro_batch_size_per_gpu: 4
  
  # Rollout configuration
  rollout:
    name: vllm  # supported as of v0.4.0
    rollout_mode: async
    tensor_model_parallel_size: 2

    max_model_len: 10000  # the max length of the rollout
    
    multi_turn:
      enable: true
      tool_config_path: configs/tools.yaml
    
    # Generation parameters
    n: 4  # Group sampling for GRPO (must be > 1)
    response_length: 7168
    max_num_batched_tokens: 4096
    temperature: 0.7
    top_p: 0.9

    # Batch sizes for rollout
    log_prob_micro_batch_size_per_gpu: 4
    
    # Performance optimization
    enforce_eager: false
    free_cache_engine: false
    enable_tokenization_sanity_check: true
    enable_chunked_prefill: false  # TODO: enable this

# Algorithm configuration
algorithm:
  # GRPO algorithm
  adv_estimator: grpo
  norm_adv_by_std_in_grpo: false
  
  # GAE parameters
  gamma: 1.0
  lam: 1.0

# Custom reward function
custom_reward_function:
  path: reward.py
  name: compute_score
  enable_custom: true 

# Disable reward model (using custom reward)
reward_model:
  enable: false